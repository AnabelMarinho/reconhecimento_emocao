{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization, Activation, Add, Input, AveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho para o arquivo zip contendo os dados\n",
    "zip_file_path = '/content/drive/MyDrive/dataset.zip'\n",
    "# diretório de destino para extrair os dados\n",
    "extracted_dir_path = '/content/datasetNormal'\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Extrair o arquivo zip\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_dir_path)\n",
    "\n",
    "# Listar os arquivos no diretório de extração\n",
    "extracted_files = os.listdir(extracted_dir_path)\n",
    "print(f'Arquivos extraídos: {extracted_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import scipy.stats\n",
    "import tensorflow as tf\n",
    "from keras import applications, optimizers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "folder = '/content/datasetNormal/train'\n",
    "print(folder)\n",
    "#mudei essas linhas de baixo pois estava em 120\n",
    "image_width = 48\n",
    "image_height = 48\n",
    "\n",
    "channels = 1\n",
    "\n",
    "train_files = []\n",
    "i=0\n",
    "for coin in ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']:\n",
    "    onlyfiles = [f for f in os.listdir(os.path.join(folder, coin)) if os.path.isfile(os.path.join(folder, coin, f))]\n",
    "    for _file in onlyfiles:\n",
    "        train_files.append(_file)\n",
    "\n",
    "dataset = np.ndarray(shape=(len(train_files), image_height, image_width, channels),\n",
    "                     dtype=np.float32)\n",
    "y_dataset = []\n",
    "\n",
    "i = 0\n",
    "for coin in ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']:\n",
    "    onlyfiles = [f for f in os.listdir(os.path.join(folder, coin)) if os.path.isfile(os.path.join(folder, coin, f))]\n",
    "    for _file in onlyfiles:\n",
    "        img_path = os.path.join(folder, coin, _file)\n",
    "        img = load_img(img_path, target_size=(image_height, image_width), color_mode='grayscale')\n",
    "        x = img_to_array(img)\n",
    "        dataset[i] = x\n",
    "        mapping = {'angry': 0 , 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
    "        y_dataset.append(mapping[coin])\n",
    "        i += 1\n",
    "        if i == 30000:\n",
    "            print(\"%d images to array\" % i)\n",
    "            break\n",
    "\n",
    "print(\"All images to array!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "dataset = dataset.astype('float32')\n",
    "dataset /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = ['Raiva', 'Nojo', 'Medo', 'Felicidade', 'Neutro', 'Tristeza', 'Surpresa']\n",
    "\n",
    "# Dicionário para armazenar o índice da primeira imagem de cada classe\n",
    "first_image_index = {}\n",
    "\n",
    "# Encontra o índice da primeira imagem de cada classe\n",
    "for i, label in enumerate(y_dataset):\n",
    "    if label not in first_image_index:\n",
    "        first_image_index[label] = i\n",
    "\n",
    "# Configura a grade para exibir as imagens\n",
    "num_classes = len(set(y_dataset))\n",
    "num_images_per_class = 1\n",
    "num_cols = num_classes\n",
    "num_rows = num_images_per_class\n",
    "\n",
    "# Cria uma figura com uma grade de subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5))\n",
    "\n",
    "# Loop através de cada classe\n",
    "for i in range(num_classes):\n",
    "    # Obtém o índice da primeira imagem da classe\n",
    "    idx = first_image_index[i]\n",
    "\n",
    "    # Obtém a imagem e converte para RGB\n",
    "    pixels = dataset[idx].reshape(image_height, image_width)\n",
    "\n",
    "    # Exibe a imagem no subplot correspondente\n",
    "    axes[i].imshow(pixels, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "    # Adiciona um título para o subplot com o rótulo\n",
    "    axes[i].set_title(f'{classes[i]}')\n",
    "\n",
    "# Exibe a figura\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical values\n",
    "n_classes = len(set(y_dataset))\n",
    "print(n_classes)\n",
    "\n",
    "y_dataset_ = to_categorical(y_dataset, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim dataset to match the size of y_dataset_\n",
    "dataset_trimmed = dataset[:len(y_dataset_)]\n",
    "\n",
    "# Splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_trimmed, y_dataset_, test_size=0.2)\n",
    "\n",
    "print(\"Train set size: {0}, Test set size: {1}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Criar listas vazias para armazenar as amostras balanceadas\n",
    "balanced_X_train = []\n",
    "balanced_y_train = []\n",
    "\n",
    "# Determinar o número de amostras na classe majoritária\n",
    "majority_samples = 7000\n",
    "\n",
    "# Iterar sobre cada classe\n",
    "for class_label in np.unique(y_train.argmax(axis=1)):\n",
    "    # Filtrar amostras pertencentes a essa classe\n",
    "    X_class = X_train[y_train.argmax(axis=1) == class_label]\n",
    "    y_class = y_train[y_train.argmax(axis=1) == class_label]\n",
    "\n",
    "    # Calcular o número de amostras na classe menos representada\n",
    "    minority_samples = len(X_class)\n",
    "\n",
    "    # Balancear as amostras aumentando a classe menos representada\n",
    "    balanced_X_class, balanced_y_class = resample(X_class, y_class,\n",
    "                                                  replace=True,\n",
    "                                                  n_samples=majority_samples,\n",
    "                                                  random_state=42)\n",
    "\n",
    "    # Adicionar amostras balanceadas à lista\n",
    "    balanced_X_train.extend(balanced_X_class)\n",
    "    balanced_y_train.extend(balanced_y_class)\n",
    "\n",
    "# Converter listas em arrays numpy\n",
    "balanced_X_train = np.array(balanced_X_train)\n",
    "balanced_y_train = np.array(balanced_y_train)\n",
    "\n",
    "# Embaralhar amostras\n",
    "shuffled_indices = np.arange(len(balanced_X_train))\n",
    "np.random.shuffle(shuffled_indices)\n",
    "balanced_X_train = balanced_X_train[shuffled_indices]\n",
    "balanced_y_train = balanced_y_train[shuffled_indices]\n",
    "\n",
    "# Verificar o tamanho dos conjuntos de dados balanceados\n",
    "print(\"Tamanho do conjunto de treinamento balanceado:\", len(balanced_X_train))\n",
    "print(\"Tamanho do conjunto de teste:\", len(X_test))\n",
    "\n",
    "for class_label in np.unique(balanced_y_train.argmax(axis=1)):\n",
    "    count = np.sum(balanced_y_train.argmax(axis=1) == class_label)\n",
    "    print(f\"Classe {class_label}: {count} amostras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization\n",
    "\n",
    "# Criar o modelo\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(image_height, image_width, 1)))\n",
    "model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
    "\n",
    "model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
    "model.add(Dense(7, activation='softmax'))  # Especifica 'softmax' como a função de ativação\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configurar EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# Treinar o modelo\n",
    "history = model.fit(balanced_X_train, balanced_y_train,validation_split= 0.2, epochs=150, callbacks=[early_stopping], batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessar a lista de valores de validação de acurácia no histórico\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Calcular a média da validação de acurácia\n",
    "mean_val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "print(\"Média da validação de acurácia:\", mean_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "history_salvo = pd.DataFrame(history.history)\n",
    "history_salvo.to_csv('history_salvo90valAcc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model structure in jason file\n",
    "model_json = model.to_json()\n",
    "with open(\"emotion_modelcnn90valAcc.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save('modelo_cnn90valAcc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar a perda durante o treinamento\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "# Obter índices das classes previstas\n",
    "preds_classes = np.argmax(preds, axis=1)\n",
    "# Obter índices das classes verdadeiras\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calcular a acurácia\n",
    "accuracy = np.mean(preds_classes == true_classes)\n",
    "print(\"Acurácia do modelo:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "for t in range(10):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for i in range(n*t, n*(t+1)):\n",
    "        plt.subplot(1, n, i + 1 - n*t)\n",
    "        plt.imshow(cv2.cvtColor(X_test[i], cv2.COLOR_BGR2RGB), cmap='gray')\n",
    "        plt.title('Label: {}\\nPredicted: {}'.format(np.argmax(y_test[i]), np.argmax(preds[i])))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "y_test_ = [np.argmax(x) for x in y_test]\n",
    "preds_ = [np.argmax(x) for x in preds]\n",
    "\n",
    "cm = confusion_matrix(y_test_, preds_)\n",
    "plt.figure(figsize= (12,12))\n",
    "plot_confusion_matrix(cm, classes=['raiva', 'nojo', 'medo', 'feliz', 'neutro', 'triste', 'surpreso']\n",
    ", title='Confusion matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
